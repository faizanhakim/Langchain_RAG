{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a515336e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import langchain\n",
    "import langsmith\n",
    "import chromadb\n",
    "import asyncio\n",
    "import string\n",
    "import random\n",
    "import re\n",
    "\n",
    "load_dotenv()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4def135d",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_ID = \"mistralai/Mistral-7B-Instruct-v0.3\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91c34f67",
   "metadata": {},
   "source": [
    "### **Text Extraction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6cba1f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pdfminer.high_level import extract_text\n",
    "\n",
    "def extract_and_clean_pdf(file_path, min_chunk_length=10):\n",
    "    \n",
    "    try:\n",
    "        raw_text = extract_text(file_path)\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting text from {file_path}: {e}\")\n",
    "        return []\n",
    "\n",
    "    text = re.sub(r'(\\w)-\\s*\\n\\s*(\\w)', r'\\1\\2', raw_text)\n",
    "\n",
    "    # 3. Split the text into potential chunks based on double newlines\n",
    "    raw_chunks = text.split('\\n\\n')\n",
    "    \n",
    "    cleaned_chunks = []\n",
    "    for chunk in raw_chunks:\n",
    "        # 4. Clean up each individual chunk\n",
    "        # Collapse single newlines and multiple spaces\n",
    "        cleaned_chunk = re.sub(r'\\s*\\n\\s*', ' ', chunk).strip()\n",
    "        cleaned_chunk = re.sub(r'\\s+', ' ', cleaned_chunk)\n",
    "\n",
    "        # 5. Filter out unwanted chunks based on generic rules\n",
    "\n",
    "        # a) Filter out short chunks that are likely headers, footers, or noise\n",
    "        if len(cleaned_chunk) < min_chunk_length:\n",
    "            continue\n",
    "\n",
    "        # b) Filter out chunks that resemble table of contents entries (e.g., \"Introduction ..... 5\")\n",
    "        if re.search(r'\\.{5,}|_{5,}', cleaned_chunk):\n",
    "            continue\n",
    "            \n",
    "        # c) Filter out chunks that are likely just page numbers or simple headers/footers\n",
    "        # This checks if a chunk has a very low ratio of alphabetic characters\n",
    "        if len(cleaned_chunk) > 0 and sum(c.isalpha() for c in cleaned_chunk) / len(cleaned_chunk) < 0.6:\n",
    "            continue\n",
    "            \n",
    "        # d) Filter out common academic/report metadata lines\n",
    "        if re.match(r'^(DOI|ISBN|ISSN):', cleaned_chunk, re.IGNORECASE):\n",
    "            continue\n",
    "\n",
    "        # If the chunk passes all filters, add it to the list\n",
    "        cleaned_chunks.append(cleaned_chunk)\n",
    "            \n",
    "    return cleaned_chunks    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a601c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = 'data/bams-d-11-00197.1.pdf'\n",
    "pdf_text = extract_and_clean_pdf(data_path)\n",
    "pdf_text[0:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dda38cf",
   "metadata": {},
   "source": [
    "### **Text Chunking**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba14a739",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=300, chunk_overlap=50)\n",
    "chunks = splitter.split_text(''.join(pdf_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efcf23c0",
   "metadata": {},
   "source": [
    "### **Text Embeddings and Vector Database**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af448b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "import chromadb\n",
    "from chromadb.utils import embedding_functions\n",
    "from sentence_transformers import SentenceTransformer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08516b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_function = embedding_functions.SentenceTransformerEmbeddingFunction(\"sentence-transformers/all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a3a9463",
   "metadata": {},
   "outputs": [],
   "source": [
    "chroma_client = chromadb.Client() # Can be swapped for PersistentClient\n",
    "collection = chroma_client.create_collection(name='weather_client_nomral_documents', embedding_function=embedding_function, get_or_create=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d93908c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = [str(uuid.uuid4()) for _ in chunks]\n",
    "meta_data = [{\"chunk_number\": chunk_num} for chunk_num in range(len(chunks))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ad2744",
   "metadata": {},
   "outputs": [],
   "source": [
    "collection.add(\n",
    "    ids=ids,\n",
    "    documents=chunks,\n",
    "    metadatas=meta_data\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3a3ea0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "collection.get(random.choice(ids), include=['embeddings'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b4f535e",
   "metadata": {},
   "source": [
    "### **Query Refinement**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d03bc7d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_huggingface import HuggingFaceEndpoint, ChatHuggingFace\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39511bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_and_clean_output(text: str) -> list[str]:\n",
    "    cleaned_text = re.sub(r\"<think>.*?</think>\", \"\", text, flags=re.DOTALL).strip()\n",
    "    questions = [q.strip() for q in cleaned_text.split(\"\\n\") if q.strip()]\n",
    "    return questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e53babd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"You are an AI language model assistant. Your task is to generate five\n",
    "different versions of the given user question to retrieve relevant documents from a vector\n",
    "database. By generating multiple perspectives on the user question, your goal is to help\n",
    "the user overcome some of the limitations of the distance-based similarity search.\n",
    "Provide these alternative questions separated by newlines.\n",
    "\n",
    "Original question: {question}\"\"\"\n",
    "\n",
    "prompt_perspectives = PromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9178c062",
   "metadata": {},
   "outputs": [],
   "source": [
    "HUGGING_FACE_API_TOKEN = os.getenv(\"HUGGING_FACE_HUB_TOKEN\")\n",
    "\n",
    "llm_endpoint = HuggingFaceEndpoint(\n",
    "    repo_id=MODEL_ID,\n",
    "    huggingfacehub_api_token=HUGGING_FACE_API_TOKEN,\n",
    "    task=\"conversational\" \n",
    ")\n",
    "\n",
    "# 3. Define the LLM using the modern Hugging Face Endpoint\n",
    "chat_model = ChatHuggingFace(llm=llm_endpoint)\n",
    "\n",
    "# 4. Create the query-generation pipeline using LCEL (LangChain Expression Language)\n",
    "generate_queries = (\n",
    "    prompt_perspectives\n",
    "    | chat_model\n",
    "    | StrOutputParser()\n",
    "    | RunnableLambda(parse_and_clean_output)\n",
    ")\n",
    "\n",
    "# 5. Invoke the pipeline\n",
    "question = \"What percentage of coral reefs are projected to decline at a global warming level of 1.5°C?\"\n",
    "response = generate_queries.invoke({\"question\": question})\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64c14bb5",
   "metadata": {},
   "source": [
    "### **Document Retrival**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5426af4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_classic.retrievers import MultiQueryRetriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdd0ee9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "langchain_embedding_function = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "954ef923",
   "metadata": {},
   "outputs": [],
   "source": [
    "langchain_chroma_store = Chroma(\n",
    "    client=chroma_client,\n",
    "    collection_name=\"weather_client_nomral_documents\",\n",
    "    embedding_function=langchain_embedding_function\n",
    ")\n",
    "\n",
    "base_retriever = langchain_chroma_store.as_retriever()\n",
    "print(\"LangChain retriever created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a8e92e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = MultiQueryRetriever(\n",
    "    retriever=base_retriever,\n",
    "    llm_chain=generate_queries\n",
    ")\n",
    "\n",
    "question = \"What percentage of coral reefs are projected to decline at a global warming level of 1.5°C?\"\n",
    "response_docs = retriever.invoke({\"question\": question}) # Pass as dict for the prompt\n",
    "\n",
    "print(\"\\n--- Retrieved Documents ---\")\n",
    "print(response_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc150f64",
   "metadata": {},
   "source": [
    "### **Ranking of Documents**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a2b807",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_classic.load import dumps, loads\n",
    "\n",
    "def rank_documents(results: list[list], k=60):\n",
    "    fused_scores = {}\n",
    "    for docs in results:\n",
    "        for rank, doc in enumerate(docs):\n",
    "            doc_str = dumps(doc)\n",
    "            if doc_str not in fused_scores:\n",
    "                fused_scores[doc_str] = 0\n",
    "            previous_score = fused_scores[doc_str]\n",
    "            fused_scores[doc_str] += 1 / (rank + k)\n",
    "\n",
    "    reranked_results = [\n",
    "        (loads(doc), score)\n",
    "        for doc, score in sorted(fused_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    ]\n",
    "\n",
    "    return reranked_results\n",
    "\n",
    "retrieval_chain = generate_queries | retriever.map() | rank_documents\n",
    "docs = retrieval_chain.invoke(question)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cbd153e",
   "metadata": {},
   "source": [
    "### **LLM Query**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a0da67b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "template = \"\"\"Answer the following question based on this context:\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "# Create templatized prompt\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "# Pass context and question into prompt, then pass prompt to LLM\n",
    "final_rag_chain = (\n",
    "    {\"context\": retrieval_chain, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | chat_model\n",
    "    | StrOutputParser()\n",
    "    | RunnableLambda(parse_and_clean_output)\n",
    ")\n",
    "\n",
    "final_rag_chain.invoke({\"question\":question})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d83549b1",
   "metadata": {},
   "source": [
    "### **RAG Evaluation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f73b627a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith import Client\n",
    "from langsmith.schemas import Run, Example\n",
    "from langsmith.evaluation import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f5d25be",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "\n",
    "\n",
    "dataset_inputs = [\n",
    "    \"What are Climate Normals?\",\n",
    "    \"What is the new set of NOAA's climate normals?\",\n",
    "    \"What are the overarching goals of NOAA's 1981-2010 U.S. Climate Normals?\",\n",
    "    \"What are the three major product lines in the new normals?\",\n",
    "    \"What is the history of NOAA's climate normals?\",\n",
    "    \"When did NOAA's NCDC become the official archive for weather records?\",\n",
    "    \"Who is NCDC's official source for calculations of U.S. normals?\",\n",
    "    \"When are climatological standard normals computed?\",\n",
    "    \"What are the products included in the 1981-2010 Climate Normals?\",\n",
    "    \"What are the temperature-related normals?\",\n",
    "    \"What are precipitation-related climate normals?\",\n",
    "    \"What is the methodological overview for the 1981-2010 climate normals?\",\n",
    "    \"How are higher-quality monthly data achieved in the 1981-2010 normals?\",\n",
    "    \"What are quasi normals for short-record stations?\"\n",
    "]\n",
    "\n",
    "dataset_outputs = [\n",
    "    {\"answer\": \"Climate normals are 30-year averages of meteorological conditions, such as air temperature and precipitation. They characterize the background state of the climate.\"},\n",
    "    {\"answer\": \"The new set of NOAA's climate normals is the 1981-2010 set, which replaces the 1971-2000 normals.\"},\n",
    "    {\"answer\": \"The goals include producing high-quality normals for many U.S. stations, being representative of the 1981-2010 period, reflecting station locations and observing practices at the end of 2010, adding new products, developing new statistical techniques, and providing timely access.\"},\n",
    "    {\"answer\": \"The three major product lines are temperature-related, precipitation-related, and hourly normals.\"},\n",
    "    {\"answer\": \"NOAA's NCDC is responsible for recording U.S. climatic conditions, stemming from the Organic Act of 1890. The WMO set guidelines for 30-year periods, and NOAA has been computing decennial 30-year normals since the 1921-50 period.\"},\n",
    "    {\"answer\": \"The Federal Records Act of 1950 established NOAA's NCDC as the official archive for weather records.\"},\n",
    "    {\"answer\": \"NOAA's National Climatic Data Center (NCDC) is the official source for calculations of U.S. normals.\"},\n",
    "    {\"answer\": \"Climatological standard normals are computed every 30 years as part of an international effort led by the WMO. Standard normals for 1901-30, 1931-60, and 1961-90 have been distributed.\"},\n",
    "    {\"answer\": \"Products include station-based temperature, precipitation, snowfall, and snow depth normals at daily, monthly, seasonal, and annual scales, as well as degree days and threshold exceedance frequencies.\"},\n",
    "    {\"answer\": \"Temperature-related normals are based on daily observations of maximum temperature (Tmax) and minimum temperature (Tmin). They include normals for Tmax, Tmin, mean temperature (Tavg), diurnal temperature range (DTR), heating degree days (HDDs), cooling degree days (CDDs), and threshold exceedance frequencies.\"},\n",
    "    {\"answer\": \"Precipitation-related normals (precipitation, snowfall, snow depth) are based on daily observations. They include monthly, seasonal, and annual averages, month-to-date and year-to-date normals, threshold exceedance frequencies, and percentiles.\"},\n",
    "    {\"answer\": \"The values come from the Global Historical Climatology Network-Daily (GHCN-Daily) dataset, which undergoes extensive quality assurance. Data flagged as erroneous are treated as missing. A station needs at least 10 'sufficiently complete' months for each month of the year.\"},\n",
    "    {\"answer\": \"The 1981-2010 normals use monthly temperature data (Tmax and Tmin) that undergo robust QA and homogenization using a pairwise comparison technique, which is then passed down to the daily time scale.\"},\n",
    "    {\"answer\": \"Quasi normals are estimated normals for active short-record stations (at least 2 years of complete months) that fail the 10-year completeness criterion. They are estimated using linear combinations of normals from neighboring longer-record stations.\"}\n",
    "]\n",
    "\n",
    "\n",
    "client = Client()\n",
    "\n",
    "dataset = client.create_dataset(\n",
    "    dataset_name=\"NOAA Climate Normals Questions\",\n",
    "    description=\"NOAA Climate Normals questions for RAG pipeline evaluation.\",\n",
    ")\n",
    "\n",
    "client.create_examples(\n",
    "    inputs=[{\"question\": q} for q in dataset_inputs],\n",
    "    outputs=dataset_outputs,\n",
    "    dataset_id=dataset.id,\n",
    ")\n",
    "\n",
    "print(f\"Successfully created dataset '{dataset.name}' with {len(dataset_inputs)} examples.\")\n",
    "print(f\"View it in LangSmith: {dataset.url}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "291f470a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 4. Define Custom Evaluator ---\n",
    "def must_mention(run: Run, example: Example) -> dict:\n",
    "    \"\"\"\n",
    "    Checks if the model's output contains any of the required phrases.\n",
    "    \"\"\"\n",
    "    # Get the RAG chain's output\n",
    "    prediction = run.outputs.get(\"output\") or \"\"\n",
    "    \n",
    "    # Get the \"ground truth\" required phrases from the dataset\n",
    "    required = example.outputs.get(\"must_mention\") or []\n",
    "    \n",
    "    # Score is 1 (True) if ANY required phrase is in the prediction, 0 (False) otherwise\n",
    "    score = any(phrase.lower() in prediction.lower() for phrase in required)\n",
    "    \n",
    "    return {\"key\": \"must_mention\", \"score\": int(score)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a25f773",
   "metadata": {},
   "outputs": [],
   "source": [
    "runner = final_rag_chain \n",
    "dataset_name = \"NOAA Climate Normals Questions\"\n",
    "\n",
    "def query_wrapper(query_dict: dict) -> dict:\n",
    "    response = runner.invoke(query_dict) \n",
    "    return {\"output\": response}\n",
    "\n",
    "\n",
    "evaluators = [must_mention]\n",
    "\n",
    "print(f\"Starting evaluation on dataset: {dataset_name}...\")\n",
    "\n",
    "experiment_results = evaluate(\n",
    "    query_wrapper,         # The function to test (your RAG chain)\n",
    "    data=dataset_name,     # The dataset to test against\n",
    "    evaluators=evaluators, # The list of grading functions\n",
    "    experiment_prefix=\"noaa-rag-pipeline\", # A name for the test run\n",
    "    client=client,\n",
    ")\n",
    "\n",
    "print(\"\\n--- Evaluation Complete ---\")\n",
    "print(experiment_results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myRAG",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
